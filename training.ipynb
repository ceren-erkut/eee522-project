{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Geng(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Geng, self).__init__()\n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 8, out_channels = 64 , kernel_size = 3 , stride = 1)\n",
    "        # In paper, they feed 1x10 downsample emg (kernel=3, filter = 64)\n",
    "\n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels = 64 , out_channels = 64 , kernel_size = 1 ,stride = 1)        \n",
    "\n",
    "        \n",
    "        # layer 3      \n",
    "        self.linear1 = nn.Linear( 246016 , 512 )\n",
    "        # layer 4\n",
    "        self.linear2 = nn.Linear( 512 , 512 )\n",
    "        # layer 5\n",
    "        self.linear3 = nn.Linear( 512 , 128 )\n",
    "\n",
    "      \n",
    "        \n",
    "\n",
    "        self.linear4 = nn.Linear( 128 , 4)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # layer 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        # layer 3\n",
    "\n",
    "        x = self.linear1( x )\n",
    "        \n",
    "        # layer 4\n",
    "        x = self.linear2( x )\n",
    "        \n",
    "        # layer 5\n",
    "        x = self.linear3( x )\n",
    "\n",
    "        x = self.linear4( x )\n",
    "        \n",
    "\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atzori(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Atzori, self).__init__()\n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 8, out_channels = 32 , kernel_size = 3)\n",
    "        # In paper channel size and kernel size is unclear\n",
    "        \n",
    "        # layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32 , kernel_size = 3)        \n",
    "        self.pool = nn.AvgPool2d(3, 3)\n",
    "        \n",
    "        # layer 3      \n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 64 , kernel_size = 5)        \n",
    "        self.pool2 = nn.AvgPool2d(3, 3)\n",
    "        \n",
    "        # layer 4\n",
    "        self.conv4 = nn.Conv2d(in_channels = 64, out_channels = 64 , kernel_size = (5,1) )\n",
    "        # it is spesific for kernel size (5,1)\n",
    "        \n",
    "        # layer 5\n",
    "        self.conv5 = nn.Conv2d(in_channels = 64, out_channels = 64 , kernel_size = 1 )  \n",
    "        \n",
    "        \n",
    "\n",
    "        self.linear = nn.Linear( 320 , 4)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # layer 2\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # layer 3\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # layer 4\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        # layer 5\n",
    "        x = F.relu(self.conv5(x))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear( x )\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zhai(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Zhai, self).__init__()\n",
    "        # layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 8, out_channels = 4 , kernel_size = 4)\n",
    "        # in paper, defined input size 12 channel 5x5 image after spectogram\n",
    "        \n",
    "        # layer 2      \n",
    "        self.linear1 = nn.Linear( 16 , 32 )\n",
    "        self.dropout1 = nn.Dropout(p = 0.5)\n",
    "        # layer 3\n",
    "        self.linear2 = nn.Linear( 32 , 32 )\n",
    "        self.dropout2 = nn.Dropout(p = 0.5)\n",
    "        # layer output\n",
    "        self.linear3 = nn.Linear( 32 , 4 )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "    \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # layer 2\n",
    "        x = self.dropout1( self.linear1( x ) )\n",
    "        \n",
    "        # layer 3\n",
    "        x = self.dropout2( self.linear2( x ) )\n",
    "        \n",
    "        # layer 5\n",
    "        x = self.linear3( x )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return F.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 12, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 6, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(6, 4, 2)\n",
    "        self.fc1 = nn.Linear(120, 100)\n",
    "        self.fc2 = nn.Linear(100, 84)\n",
    "        self.fc3 = nn.Linear(84, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax( self.fc3(x) )\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle( \"./ft_df_alpha_\" + str(alpha)+ \".pkl\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle( \"./ft_df_ver1_alpha_\" + str(alpha)+ \".pkl\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(df)\n",
    "num_channels = 4  # Ch1, Ch3, Ch8, Ch9, Ch10, Ch11\n",
    "height, width = df.iloc[0]['Ch1'].shape\n",
    "\n",
    "X = torch.zeros((num_samples, num_channels, height, width), dtype=torch.float)\n",
    "\n",
    "for ind in range(num_samples):\n",
    "    # X[ind, 0, :, :] = torch.tensor(df.iloc[ind]['Ch1'], dtype=torch.float)\n",
    "    # X[ind, 1, :, :] = torch.tensor(df.iloc[ind]['Ch3'], dtype=torch.float)\n",
    "    X[ind, 0, :, :] = torch.tensor(df.iloc[ind]['Ch8'], dtype=torch.float)\n",
    "    X[ind, 1, :, :] = torch.tensor(df.iloc[ind]['Ch9'], dtype=torch.float)\n",
    "    X[ind, 2, :, :] = torch.tensor(df.iloc[ind]['Ch10'], dtype=torch.float)\n",
    "    X[ind, 3, :, :] = torch.tensor(df.iloc[ind]['Ch11'], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vals = X.amin(dim=(2, 3), keepdim=True)\n",
    "max_vals = X.amax(dim=(2, 3), keepdim=True)\n",
    "X_normalized = (X - min_vals) / (max_vals - min_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b5fc5a75d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure( figsize=(3,8))\n",
    "plt.imshow( X_normalized[500,3,:,:] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEmg (12 columns): sEMG signal. Columns 1-8 are the electrodes equally spaced around the forearm at the height of the radio humeral joint. \\nColumns 9 and 10 contain signals from the main activity spot of the muscles flexor and extensor digitorum superficialis, \\nwhile columns 11 and 12 contain signals from the main activity spot of the muscles biceps brachii and triceps brachii.\\n\\n6 chs should be enough\\n2 4 9 10 11 12\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Emg (12 columns): sEMG signal. Columns 1-8 are the electrodes equally spaced around the forearm at the height of the radio humeral joint. \n",
    "Columns 9 and 10 contain signals from the main activity spot of the muscles flexor and extensor digitorum superficialis, \n",
    "while columns 11 and 12 contain signals from the main activity spot of the muscles biceps brachii and triceps brachii.\n",
    "\n",
    "6 chs should be enough\n",
    "2 4 9 10 11 12\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.get_dummies(df['y'])\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "y = torch.tensor(one_hot_df.values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check if GPU is available and get device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move the data to the GPU\n",
    "X_train, X_test = X_train.to(device), X_test.to(device)\n",
    "y_train, y_test = y_train.to(device), y_test.to(device)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(4, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 6, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (conv3): Conv2d(6, 4, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=120, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net(y.shape[1])\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erkan\\AppData\\Local\\Temp\\ipykernel_28216\\3911113333.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax( self.fc3(x) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 1.032\n",
      "[2] loss: 1.001\n",
      "[3] loss: 0.994\n",
      "[4] loss: 0.979\n",
      "[5] loss: 0.902\n",
      "[6] loss: 0.884\n",
      "[7] loss: 0.884\n",
      "[8] loss: 0.877\n",
      "[9] loss: 0.875\n",
      "[10] loss: 0.874\n",
      "[11] loss: 0.872\n",
      "[12] loss: 0.868\n",
      "[13] loss: 0.865\n",
      "[14] loss: 0.865\n",
      "[15] loss: 0.863\n",
      "[16] loss: 0.865\n",
      "[17] loss: 0.861\n",
      "[18] loss: 0.867\n",
      "[19] loss: 0.859\n",
      "[20] loss: 0.857\n",
      "[21] loss: 0.855\n",
      "[22] loss: 0.853\n",
      "[23] loss: 0.855\n",
      "[24] loss: 0.854\n",
      "[25] loss: 0.853\n",
      "[26] loss: 0.853\n",
      "[27] loss: 0.849\n",
      "[28] loss: 0.844\n",
      "[29] loss: 0.852\n",
      "[30] loss: 0.844\n",
      "[31] loss: 0.851\n",
      "[32] loss: 0.847\n",
      "[33] loss: 0.845\n",
      "[34] loss: 0.839\n",
      "[35] loss: 0.839\n",
      "[36] loss: 0.834\n",
      "[37] loss: 0.836\n",
      "[38] loss: 0.839\n",
      "[39] loss: 0.840\n",
      "[40] loss: 0.834\n",
      "[41] loss: 0.834\n",
      "[42] loss: 0.830\n",
      "[43] loss: 0.826\n",
      "[44] loss: 0.832\n",
      "[45] loss: 0.845\n",
      "[46] loss: 0.834\n",
      "[47] loss: 0.839\n",
      "[48] loss: 0.828\n",
      "[49] loss: 0.833\n",
      "[50] loss: 0.834\n",
      "[51] loss: 0.823\n",
      "[52] loss: 0.821\n",
      "[53] loss: 0.816\n",
      "[54] loss: 0.823\n",
      "[55] loss: 0.818\n",
      "[56] loss: 0.813\n",
      "[57] loss: 0.811\n",
      "[58] loss: 0.817\n",
      "[59] loss: 0.815\n",
      "[60] loss: 0.808\n",
      "[61] loss: 0.811\n",
      "[62] loss: 0.804\n",
      "[63] loss: 0.807\n",
      "[64] loss: 0.821\n",
      "[65] loss: 0.811\n",
      "[66] loss: 0.807\n",
      "[67] loss: 0.802\n",
      "[68] loss: 0.809\n",
      "[69] loss: 0.808\n",
      "[70] loss: 0.802\n",
      "[71] loss: 0.798\n",
      "[72] loss: 0.797\n",
      "[73] loss: 0.797\n",
      "[74] loss: 0.798\n",
      "[75] loss: 0.799\n",
      "[76] loss: 0.809\n",
      "[77] loss: 0.819\n",
      "[78] loss: 0.795\n",
      "[79] loss: 0.793\n",
      "[80] loss: 0.819\n",
      "[81] loss: 0.802\n",
      "[82] loss: 0.796\n",
      "[83] loss: 0.794\n",
      "[84] loss: 0.799\n",
      "[85] loss: 0.806\n",
      "[86] loss: 0.794\n",
      "[87] loss: 0.791\n",
      "[88] loss: 0.810\n",
      "[89] loss: 0.793\n",
      "[90] loss: 0.786\n",
      "[91] loss: 0.787\n",
      "[92] loss: 0.785\n",
      "[93] loss: 0.786\n",
      "[94] loss: 0.785\n",
      "[95] loss: 0.796\n",
      "[96] loss: 0.791\n",
      "[97] loss: 0.785\n",
      "[98] loss: 0.788\n",
      "[99] loss: 0.792\n",
      "[100] loss: 0.787\n",
      "[101] loss: 0.779\n",
      "[102] loss: 0.778\n",
      "[103] loss: 0.783\n",
      "[104] loss: 0.797\n",
      "[105] loss: 0.796\n",
      "[106] loss: 0.788\n",
      "[107] loss: 0.793\n",
      "[108] loss: 0.786\n",
      "[109] loss: 0.780\n",
      "[110] loss: 0.792\n",
      "[111] loss: 0.794\n",
      "[112] loss: 0.781\n",
      "[113] loss: 0.789\n",
      "[114] loss: 0.789\n",
      "[115] loss: 0.779\n",
      "[116] loss: 0.781\n",
      "[117] loss: 0.779\n",
      "[118] loss: 0.778\n",
      "[119] loss: 0.778\n",
      "[120] loss: 0.799\n",
      "[121] loss: 0.788\n",
      "[122] loss: 0.808\n",
      "[123] loss: 0.781\n",
      "[124] loss: 0.779\n",
      "[125] loss: 0.793\n",
      "[126] loss: 0.780\n",
      "[127] loss: 0.776\n",
      "[128] loss: 0.783\n",
      "[129] loss: 0.798\n",
      "[130] loss: 0.791\n",
      "[131] loss: 0.797\n",
      "[132] loss: 0.779\n",
      "[133] loss: 0.781\n",
      "[134] loss: 0.784\n",
      "[135] loss: 0.778\n",
      "[136] loss: 0.778\n",
      "[137] loss: 0.782\n",
      "[138] loss: 0.782\n",
      "[139] loss: 0.783\n",
      "[140] loss: 0.788\n",
      "[141] loss: 0.792\n",
      "[142] loss: 0.823\n",
      "[143] loss: 0.792\n",
      "[144] loss: 0.786\n",
      "[145] loss: 0.785\n",
      "[146] loss: 0.784\n",
      "[147] loss: 0.777\n",
      "[148] loss: 0.780\n",
      "[149] loss: 0.773\n",
      "[150] loss: 0.772\n",
      "[151] loss: 0.776\n",
      "[152] loss: 0.786\n",
      "[153] loss: 0.779\n",
      "[154] loss: 0.768\n",
      "[155] loss: 0.769\n",
      "[156] loss: 0.769\n",
      "[157] loss: 0.770\n",
      "[158] loss: 0.769\n",
      "[159] loss: 0.771\n",
      "[160] loss: 0.775\n",
      "[161] loss: 0.768\n",
      "[162] loss: 0.772\n",
      "[163] loss: 0.778\n",
      "[164] loss: 0.774\n",
      "[165] loss: 0.778\n",
      "[166] loss: 0.771\n",
      "[167] loss: 0.770\n",
      "[168] loss: 0.767\n",
      "[169] loss: 0.770\n",
      "[170] loss: 0.766\n",
      "[171] loss: 0.774\n",
      "[172] loss: 0.764\n",
      "[173] loss: 0.786\n",
      "[174] loss: 0.770\n",
      "[175] loss: 0.769\n",
      "[176] loss: 0.771\n",
      "[177] loss: 0.762\n",
      "[178] loss: 0.762\n",
      "[179] loss: 0.764\n",
      "[180] loss: 0.762\n",
      "[181] loss: 0.761\n",
      "[182] loss: 0.767\n",
      "[183] loss: 0.766\n",
      "[184] loss: 0.769\n",
      "[185] loss: 0.766\n",
      "[186] loss: 0.771\n",
      "[187] loss: 0.778\n",
      "[188] loss: 0.786\n",
      "[189] loss: 0.778\n",
      "[190] loss: 0.784\n",
      "[191] loss: 0.782\n",
      "[192] loss: 0.777\n",
      "[193] loss: 0.764\n",
      "[194] loss: 0.764\n",
      "[195] loss: 0.767\n",
      "[196] loss: 0.769\n",
      "[197] loss: 0.770\n",
      "[198] loss: 0.764\n",
      "[199] loss: 0.761\n",
      "[200] loss: 0.759\n",
      "[201] loss: 0.771\n",
      "[202] loss: 0.768\n",
      "[203] loss: 0.768\n",
      "[204] loss: 0.760\n",
      "[205] loss: 0.760\n",
      "[206] loss: 0.759\n",
      "[207] loss: 0.762\n",
      "[208] loss: 0.759\n",
      "[209] loss: 0.758\n",
      "[210] loss: 0.759\n",
      "[211] loss: 0.766\n",
      "[212] loss: 0.764\n",
      "[213] loss: 0.757\n",
      "[214] loss: 0.755\n",
      "[215] loss: 0.762\n",
      "[216] loss: 0.759\n",
      "[217] loss: 0.761\n",
      "[218] loss: 0.757\n",
      "[219] loss: 0.773\n",
      "[220] loss: 0.777\n",
      "[221] loss: 0.767\n",
      "[222] loss: 0.771\n",
      "[223] loss: 0.771\n",
      "[224] loss: 0.771\n",
      "[225] loss: 0.766\n",
      "[226] loss: 0.772\n",
      "[227] loss: 0.763\n",
      "[228] loss: 0.768\n",
      "[229] loss: 0.761\n",
      "[230] loss: 0.771\n",
      "[231] loss: 0.783\n",
      "[232] loss: 0.765\n",
      "[233] loss: 0.766\n",
      "[234] loss: 0.758\n",
      "[235] loss: 0.757\n",
      "[236] loss: 0.758\n",
      "[237] loss: 0.756\n",
      "[238] loss: 0.757\n",
      "[239] loss: 0.756\n",
      "[240] loss: 0.756\n",
      "[241] loss: 0.758\n",
      "[242] loss: 0.756\n",
      "[243] loss: 0.755\n",
      "[244] loss: 0.772\n",
      "[245] loss: 0.780\n",
      "[246] loss: 0.768\n",
      "[247] loss: 0.764\n",
      "[248] loss: 0.760\n",
      "[249] loss: 0.765\n",
      "[250] loss: 0.768\n",
      "[251] loss: 0.774\n",
      "[252] loss: 0.765\n",
      "[253] loss: 0.763\n",
      "[254] loss: 0.757\n",
      "[255] loss: 0.769\n",
      "[256] loss: 0.761\n",
      "[257] loss: 0.759\n",
      "[258] loss: 0.762\n",
      "[259] loss: 0.756\n",
      "[260] loss: 0.761\n",
      "[261] loss: 0.755\n",
      "[262] loss: 0.754\n",
      "[263] loss: 0.757\n",
      "[264] loss: 0.758\n",
      "[265] loss: 0.758\n",
      "[266] loss: 0.755\n",
      "[267] loss: 0.765\n",
      "[268] loss: 0.760\n",
      "[269] loss: 0.759\n",
      "[270] loss: 0.759\n",
      "[271] loss: 0.760\n",
      "[272] loss: 0.757\n",
      "[273] loss: 0.762\n",
      "[274] loss: 0.768\n",
      "[275] loss: 0.760\n",
      "[276] loss: 0.762\n",
      "[277] loss: 0.767\n",
      "[278] loss: 0.761\n",
      "[279] loss: 0.754\n",
      "[280] loss: 0.754\n",
      "[281] loss: 0.755\n",
      "[282] loss: 0.759\n",
      "[283] loss: 0.756\n",
      "[284] loss: 0.753\n",
      "[285] loss: 0.752\n",
      "[286] loss: 0.757\n",
      "[287] loss: 0.752\n",
      "[288] loss: 0.763\n",
      "[289] loss: 0.756\n",
      "[290] loss: 0.756\n",
      "[291] loss: 0.762\n",
      "[292] loss: 0.757\n",
      "[293] loss: 0.753\n",
      "[294] loss: 0.753\n",
      "[295] loss: 0.750\n",
      "[296] loss: 0.752\n",
      "[297] loss: 0.757\n",
      "[298] loss: 0.758\n",
      "[299] loss: 0.757\n",
      "[300] loss: 0.757\n",
      "[301] loss: 0.765\n",
      "[302] loss: 0.787\n",
      "[303] loss: 0.748\n",
      "[304] loss: 0.740\n",
      "[305] loss: 0.752\n",
      "[306] loss: 0.739\n",
      "[307] loss: 0.741\n",
      "[308] loss: 0.742\n",
      "[309] loss: 0.748\n",
      "[310] loss: 0.750\n",
      "[311] loss: 0.737\n",
      "[312] loss: 0.740\n",
      "[313] loss: 0.736\n",
      "[314] loss: 0.737\n",
      "[315] loss: 0.742\n",
      "[316] loss: 0.750\n",
      "[317] loss: 0.738\n",
      "[318] loss: 0.735\n",
      "[319] loss: 0.745\n",
      "[320] loss: 0.741\n",
      "[321] loss: 0.746\n",
      "[322] loss: 0.761\n",
      "[323] loss: 0.739\n",
      "[324] loss: 0.737\n",
      "[325] loss: 0.732\n",
      "[326] loss: 0.730\n",
      "[327] loss: 0.733\n",
      "[328] loss: 0.731\n",
      "[329] loss: 0.740\n",
      "[330] loss: 0.727\n",
      "[331] loss: 0.735\n",
      "[332] loss: 0.740\n",
      "[333] loss: 0.738\n",
      "[334] loss: 0.744\n",
      "[335] loss: 0.735\n",
      "[336] loss: 0.735\n",
      "[337] loss: 0.754\n",
      "[338] loss: 0.741\n",
      "[339] loss: 0.739\n",
      "[340] loss: 0.727\n",
      "[341] loss: 0.731\n",
      "[342] loss: 0.724\n",
      "[343] loss: 0.728\n",
      "[344] loss: 0.721\n",
      "[345] loss: 0.723\n",
      "[346] loss: 0.725\n",
      "[347] loss: 0.721\n",
      "[348] loss: 0.730\n",
      "[349] loss: 0.741\n",
      "[350] loss: 0.727\n",
      "[351] loss: 0.723\n",
      "[352] loss: 0.725\n",
      "[353] loss: 0.731\n",
      "[354] loss: 0.731\n",
      "[355] loss: 0.725\n",
      "[356] loss: 0.723\n",
      "[357] loss: 0.722\n",
      "[358] loss: 0.717\n",
      "[359] loss: 0.720\n",
      "[360] loss: 0.725\n",
      "[361] loss: 0.719\n",
      "[362] loss: 0.729\n",
      "[363] loss: 0.738\n",
      "[364] loss: 0.718\n",
      "[365] loss: 0.721\n",
      "[366] loss: 0.726\n",
      "[367] loss: 0.716\n",
      "[368] loss: 0.714\n",
      "[369] loss: 0.716\n",
      "[370] loss: 0.716\n",
      "[371] loss: 0.723\n",
      "[372] loss: 0.718\n",
      "[373] loss: 0.732\n",
      "[374] loss: 0.751\n",
      "[375] loss: 0.728\n",
      "[376] loss: 0.718\n",
      "[377] loss: 0.720\n",
      "[378] loss: 0.716\n",
      "[379] loss: 0.716\n",
      "[380] loss: 0.708\n",
      "[381] loss: 0.710\n",
      "[382] loss: 0.709\n",
      "[383] loss: 0.720\n",
      "[384] loss: 0.720\n",
      "[385] loss: 0.725\n",
      "[386] loss: 0.737\n",
      "[387] loss: 0.731\n",
      "[388] loss: 0.713\n",
      "[389] loss: 0.712\n",
      "[390] loss: 0.702\n",
      "[391] loss: 0.709\n",
      "[392] loss: 0.708\n",
      "[393] loss: 0.707\n",
      "[394] loss: 0.723\n",
      "[395] loss: 0.722\n",
      "[396] loss: 0.712\n",
      "[397] loss: 0.712\n",
      "[398] loss: 0.703\n",
      "[399] loss: 0.700\n",
      "[400] loss: 0.703\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Move the model to the GPU\n",
    "\n",
    "for epoch in range(400):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Move inputs and labels to the GPU\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'[{epoch + 1}] loss: {running_loss / len(train_loader):.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400] test loss: 0.896\n",
      "[400] accuracy on test set: 65.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erkan\\AppData\\Local\\Temp\\ipykernel_28216\\3911113333.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax( self.fc3(x) )\n"
     ]
    }
   ],
   "source": [
    "net.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "            \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'[{epoch + 1}] test loss: {test_loss / len(test_loader):.3f}')\n",
    "print(f'[{epoch + 1}] accuracy on test set: {100 * correct / total:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
